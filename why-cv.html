<!doctype html><html lang="en" style="background-color:#e5e5e5"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1"><link rel="shortcut icon" href="e0e9c80960d3a288ca46.png" type="image" style="height:auto;width:10px"><link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.0/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-KyZXEAg3QhqLMpG8r+8fhAXLRk2vvoC2f3B09zVXn8CA5QIVfZOJ3BCsw2P0p/We" crossorigin="anonymous"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fullPage.js/3.1.2/fullpage.min.css" integrity="sha512-4rPgyv5iG0PZw8E+oRdfN/Gq+yilzt9rQ8Yci2jJ15rAyBmF0HBE4wFjBkoB72cxBeg63uobaj1UcNt/scV93w==" crossorigin="anonymous" referrerpolicy="no-referrer"/><link href="13de1b907c64086ad6bd.css" rel="stylesheet"><link rel="stylesheet" href="32223e31301ac0fabbff.css" class="href"><link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link href="https://fonts.googleapis.com/css2?family=Poppins:wght@200;400;700&display=swap" rel="stylesheet"><link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link href="https://fonts.googleapis.com/css2?family=Nanum+Myeongjo&family=Secular+One&display=swap" rel="stylesheet"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" crossorigin="anonymous" referrerpolicy="no-referrer"/><title>LitWiz Labs</title><script defer="defer" src="bundle.dc0c9591fb8eafdf486f.js"></script></head><body style="background:#e5e5e5"><nav id="navver" class="navbar navbar-light navbar-expand-md bg-white py-md-4 px-4 justify-content-md-center justify-content-start" style="width:100vw"><div class="container-fluid"><a class="navbar-brand" href="#"><img src="09f434a86870ac96a6af.png" alt="" class="d-inline-block align-text-top"> </a><button class="navbar-toggler collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="container" onclick="myFunction(this)"><div class="bar1"></div><div class="bar2"></div><div class="bar3"></div></div></button><div class="collapse navbar-collapse justify-content-between align-items-center w-100" id="navbarSupportedContent"><ul class="navbar-nav mx-auto text-md-center text-center"><li class="nav-item"><a class="nav-link" href="/">HOME</a></li><li class="nav-item"><a class="nav-link focus" href="/blog.html">BLOG</a></li><li class="nav-item"><a class="nav-link" href="/portfolio.html">PORTFOLIO</a></li></ul><ul class="nav navbar-nav flex-row justify-content-md-center justify-content-center flex-nowrap"><li class="nav-item"><button class="btn btn-git btn p-3" onclick='window.location="/index.html#contactus"'>GET IN TOUCH</button></li></ul></div></div></nav><div class="container justify-content-center align-items-center pt-5"><section style="background-color:#fff;padding:3%"><div class="row p-5"><p class="card-micro-text" style="font-size:12px">RESEARCH | COMPUTER VISION</p><h1>Why should we adopt Computer Vision?</h1><div class="row" style="height:20%;color:#999"><div class="col d-flex"><label style="width:fit-content;margin-bottom:auto;margin-top:auto">July 23, 2021</label></div><div class="col d-flex align-items-center justify-content-end" style="text-align:right"><i class="fa fa-share-alt fa-2x" style="margin:20px" onclick='copyStringToClipboard(window.location.href),alert("URL copied!")'></i></div></div></div></section></div><div class="container justify-content-center align-items-center pt-5"><section style="background-color:#fff;padding:3%"><div class="row p-5 blog-content-pad"><h3>Computer Vision v/s Human Vision</h3><p class="p-3">Just like us with a pair of lenses, machines like smartphones, tablets, PCs also have a vast mechanism to see different images. But the thin line of difference between the two needs to be pondered on.<br><br>Imagine yourself coming across an object. As soon as the object comes in front of your eyes, it instantly gives a stimulus to your brain to extract all the details and all the records, memories, usage, and experiences associated with it. Hence, this is how an average human vision works.<br><br>On the other hand, this tiny lens right above your screen has a totally different view. Unlike the human eye, the camera is not getting any instant stimulus to think or analyze the details, or scan through any history. It doesn't sense or function as efficiently as that of the human brain to render any details instantly. So unless you command the system or the camera to click and analyze the picture of the object, it won't even execute any task on its own. This is how an average system discerns.</p><h3>CV, Everywhere!</h3><p class="p-3">This is exactly where the idea of Computer Vision comes into the picture. With humans evolving and learning new skills, there is a fair group of intellectuals who are trying their best to equip a machine with the level of intelligence a human possesses. Constant efforts and experiments are being done by various established institutes to achieve the goal of giving a system or a camera an ability to sense, analyze and react on its own, without any external involvement or input. Few of the honorable mentions available today include automatic automobiles, sensory devices, virtual assistants on smartphones, etc.</p><h3>The Technical Tale</h3><p class="p-3">The development of Computer Vision began back in the years of late 1960s when an expedition to see the world with the eyes of technology started. Students and teachers in universities were curious to know whether they can alter the alignments of any digital image and command the system to develop new forms on its own.</p><ul class="list-margins"><li><p style="font-style:italic">The 1950s-60s</p><p>The winters of the year 1963 were marked with the revolutionary study and experimentation of the father of Computer Vision <strong>Larry Roberts</strong>, which explained the process of deriving 3D details about a solid object from its 2D photographs. Not to mention that this major milestone was achieved after the short gap of four years from when Russell Kirsch along with this team created an apparatus that transformed images into grids of numbers, to be deciphered by the gadgets understanding binary language back in 1959.</p></li><br><br><li><p style="font-style:italic">Stepping to 1970s</p><p><strong>David Marr</strong>, a young British neuroscientist, and physiologist came up with the idea of a vision system having some sort of hierarchy, which can be used to create 3D images of the environment that could interact. This ambitious idea was inspired by the works of Wiesel and Hubel, who were the first to note that holistic objects do not appear at the beginning of vision processing. By the 1970s, the development of some of the most important systematic operations and algorithms existing today also captured its pace.</p></li><br><br><li><p style="font-style:italic">Within 1980s-90s</p><p>Where the era of 1980s didn't seem to have any major development in CV world with its stillness, the years of 1990's witnessed the growth of independent formations of images and active research in projectile 3-D reconstructions (3-D images were started to be produced, purely based on data and codes aligned in a systematic order in the machines and by putting several frames together ). The focus was largely shifted from creating 3D objects for work over, to bringing in the advancement of "object recognition" and sensory system.</p></li><br><br><li><p style="font-style:italic">2000's</p><p>After a long struggle and 'n' number of experiments, the scientists seemed to find their answers. The first structure of face recognition was settled in 2001, which after many alterations was prepared and were ready to be launched and utilized in the mainstream. More people started to have much easier access to this new advancement with each passing year.</p></li><br><br><li><p style="font-style:italic">2010 - present</p><p>By 2010, The <strong>ImageNet Large Scale Visual Recognition Competition</strong> (ILSVRC- which evaluates algorithms for face detection and image classification among the most innovative entries filled by various institutes in their event ) included a post-competition workshop where participants discuss their observations. The event recorded the rate of the error to be 26% among all. This record was later broken by a group from the University of Toronto which stepped in the competition with their convolutional neural network model (AlexNet), and left everyone in the room with awe for having the error rate dropped to 16.4%! After this, there was no turning as the quality just seemed to improve further.</p></li><br><br></ul><h3>A journey from "back " to "now‚Äù</h3><p class="p-3">Today, one can find himself surrounded by computer vision, ranging from the feature of face recognition in their brand new iPhone to giant CT scan machines, giving precise scans each time. Many industries are coming forward to work along with the control of Computer Vision like reputed security agencies that use face detection on surveillance cameras of a subway to find their prime suspects.<br><br>Thanks to consistent updates and bug fixes with the help of adjustments in CV, we can see that the applications like Canva, Inshorts, Facebook, Snapchat are in high demand as well! Million+ possibilities Though the form of information a system perceives digitally might seem like a tricky alien language to be deciphered by an average being in the first go, we have reached a long way in bringing the odds in our favor and working within the boundaries of our likings. In the end, we can say that by adopting Computer Vision, we can have a scope to develop the tendency of fast-forwarding ourselves years further in future!</p></div></section></div><section style="background-color:#e5e5e5"><div class="row"></div></section><div class="container justify-content-center align-items-center pt-5"><section><div class="row p-5"><h1>Written by</h1><h4 class="pt-3" style="color:#4c4d9e;font-weight:500">Sneha Sharma</h4><h5 style="color:#999">Content Executive at LitWiz Labs</h5></div></section></div><section style="background-color:#e5e5e5"><div class="row p-5"></div></section><footer class="" id="footer-blog"><div id="footer-l" class="container footer-div"><div class="row row-cols-2 row-cols-md-5 pt-5 pb-3"><img id="social-bottom" src="fdece2681af78fbced99.png" alt="" class="d-inline-block align-text-top" style="width:100%;height:auto;padding:50px"><br><div id="social-content" class="col pt-3 pt-md-0"><img src="fdece2681af78fbced99.png" alt="" class="d-inline-block align-text-top" style="max-width:100%;height:auto"><br><br><p id="footer-aboutus" class="mb-3 pt-3 pt-md-0">We offer ready-to-deploy visual AI solutions, augmenting human capabilities with high customization.</p><br><p id="social-content" class="social text-muted mb-0 pb-0 bold-text"><span class="mx-2"><a href="https://facebook.com/" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a> </span><span class="mx-2"><a href="https://www.linkedin.com/company/litwizlabs/" target="_blank"><i class="fa fa-linkedin-square" aria-hidden="true"></i></a> </span><span class="mx-2"><a href="https://twitter.com" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a> </span><span class="mx-2"><a href="https://instagram.com" target="_blank"><i class="fa fa-instagram" aria-hidden="true"></i></a></span></p></div><div class="col pt-md-0"><h6 class="footer-content mb-3 pt-3 pt-md-1" style="font-weight:600;letter-spacing:1px">Contact</h6><p class="footer-content mb-3 pt-3 pt-md-1">HSR Layout, Bengaluru, Karnataka - 560102<br>+91 9711 283 727<br>contact@litwizlabs.ai</p></div><div class="col"><h6 class="footer-content mb-3 pt-3 pt-md-1" style="font-weight:600;letter-spacing:1px">Explore</h6><div class="list-group footer-content pt-3 pt-md-1"><a href="/index.html" class="list-group-item">Home</a> <a href="/blog.html" class="list-group-item">Blog</a> <a href="/portfolio.html" class="list-group-item">Portfolio</a></div></div><div class="col"><div class="list-group footer-content pt-3 pt-md-1"><label class="footer-btn list-group-item list-group-item" onclick="fullpage_api.moveTo(2)">What do we do?</label> <label class="footer-btn list-group-item list-group-item" onclick="fullpage_api.moveTo(3)">Products</label> <label class="footer-btn list-group-item list-group-item" onclick="fullpage_api.moveTo(4)">Why us?</label> <label class="footer-btn list-group-item list-group-item" onclick="fullpage_api.moveTo(5)">Testimonials</label></div></div><div class="col d-flex justify-content-center align-items-center"><a id="to-top-btn" href="#navver"><i class="fa fa-chevron-up fa-2x" style="color:#fff;font-size:40px"></i><h6>Back to Top</h6></a></div></div><div id="social-bottom" class="row"><p class="social text-muted mb-0 pb-0 bold-text"><span class="mx-2"><a href="https://facebook.com/" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a> </span><span class="mx-2"><a href="https://www.linkedin.com/company/litwizlabs/" target="_blank"><i class="fa fa-linkedin-square" aria-hidden="true"></i></a> </span><span class="mx-2"><a href="https://twitter.com" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a> </span><span class="mx-2"><a href="https://instagram.com" target="_blank"><i class="fa fa-instagram" aria-hidden="true"></i></a></span></p></div><div class="row"><hr class="style1"></div><div class="row footer-copyright"><p style="text-align:end;color:#ffffff7a">¬© LitWiz Labs | All rights reserved.</p></div></div><div id="footer-p" class="container footer-div"><div class="row row-cols-1 pb-3"><div id="social-content" class="d-flex flex-column col p-3 pt-md-0"><img src="fdece2681af78fbced99.png" alt="" class="d-inline-block align-text-top align-self-start" style="max-width:50%;height:auto;padding-left:10px;padding-top:10px"><br><p id="footer-aboutus" class="pt-md-0" style="color:#ffffff7a;font-size:14px;width:fit-content;margin-bottom:0!important">We offer ready-to-deploy visual AI solutions, augmenting human capabilities with high customization.</p><div class="list-group p-3 d-flex justify-content-center align-items-center" style="color:#ffffff7a"><a href="/index.html" class="list-group-item-mob">Home</a> <a href="/blog.html" class="list-group-item-mob">Blog</a> <a href="/portfolio.html" class="list-group-item-mob">Portfolio</a></div><div><p class="social text-muted mb-0 pb-0 bold-text"><span class="mx-2"><a href="https://facebook.com/" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a> </span><span class="mx-2"><a href="https://www.linkedin.com/company/litwizlabs/" target="_blank"><i class="fa fa-linkedin-square" aria-hidden="true"></i></a> </span><span class="mx-2"><a href="https://twitter.com" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a> </span><span class="mx-2"><a href="https://instagram.com" target="_blank"><i class="fa fa-instagram" aria-hidden="true"></i></a></span></p></div><div><p class="mb-3 pt-3 pt-md-1" style="color:#ffffff7a;font-size:12px;text-align:center">HSR Layout, Bengaluru, Karnataka - 560102<br>+91 9711 283 727<br>contact@litwizlabs.ai</p></div></div></div><div class="col pt-md-0"></div><div class="row"><hr class="style1"></div><div class="row footer-copyright"><p style="text-align:end;color:#ffffff7a">¬© LitWiz Labs | All rights reserved.</p></div></div></footer><script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.0/dist/js/bootstrap.min.js" integrity="sha384-cn7l7gDp0eyniUwwAZgrzD06kc/tftFf19TOAs2zVinnD/C7E91j9yyk5//jjpt/" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script><script>function copyStringToClipboard(e){var t=document.createElement("textarea");t.value=e,t.setAttribute("readonly",""),t.style={position:"absolute",left:"-9999px"},document.body.appendChild(t),t.select(),document.execCommand("copy"),document.body.removeChild(t)}</script><script>function myFunction(n){n.classList.toggle("change")}</script></body></html>